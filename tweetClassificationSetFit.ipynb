{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit\n",
    "!pip install optimum[onnxruntime-gpu] -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 5564.21 examples/s]\n",
      "***** Running training *****\n",
      "  Num unique pairs = 150\n",
      "  Batch size = 32\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.275839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.41526315789473683}\n",
      "['positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "# Remap the labels from integers to strings\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(\n",
    "        lambda example: {\"label\": id2label[example[\"label\"]]},\n",
    "        desc=f\"Remapping labels in {split} split\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Simulate the few-shot regime by sampling 8 examples per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=5)\n",
    "eval_dataset = dataset[\"validation\"].select(range(100))\n",
    "test_dataset = dataset[\"validation\"].select(range(100, len(dataset[\"validation\"])))\n",
    "\n",
    "# Load a SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    labels=[\"negative\", \"neutral\", \"positive\"],\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    #column_mapping={\"text\": \"text\", \"label\": \"label\"}  # Map dataset columns to text/label expected by trainer\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "# Run inference\n",
    "preds = model.predict([\"i loved the spiderman movie!\", \"pineapple on pizza is the worst ü§Æ\"])\n",
    "print(preds)\n",
    "# [\"positive\", \"negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference speed:\n",
      "- Average time for 100 samples: 0.0378 seconds (¬±0.0039)\n",
      "- Samples per second: 2647.50\n",
      "- Time per sample: 0.38 ms\n"
     ]
    }
   ],
   "source": [
    "# Measure inference speed of the trained model\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Prepare a list of sample texts for inference\n",
    "sample_texts = [\"i loved the spiderman movie!\"] * 100  # Create 100 identical samples for consistent measurement\n",
    "\n",
    "# Warm-up run (to avoid measuring initialization overhead)\n",
    "_ = model.predict(sample_texts[:5])\n",
    "\n",
    "# Measure inference time\n",
    "num_runs = 100\n",
    "times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(sample_texts)\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Calculate statistics\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "samples_per_second = len(sample_texts) / avg_time\n",
    "\n",
    "print(f\"Inference speed:\")\n",
    "print(f\"- Average time for {len(sample_texts)} samples: {avg_time:.4f} seconds (¬±{std_time:.4f})\")\n",
    "print(f\"- Samples per second: {samples_per_second:.2f}\")\n",
    "print(f\"- Time per sample: {(avg_time / len(sample_texts)) * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"setfit-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-09 22:30:33.780422069 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-04-09 22:30:33.784217183 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-09 22:30:33.784238531 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Overridding for_gpu=False to for_gpu=True as half precision is available only on GPU.\n",
      "/home/jan/miniconda3/envs/FFit/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:784: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2025-04-09 22:30:35.221968747 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-09 22:30:35.221991221 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "symbolic shape inference disabled or failed.\n",
      "\u001b[0;93m2025-04-09 22:30:43.031498120 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-09 22:30:43.031522686 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\t\t-[x] values not close enough, max diff: 0.007680624723434448 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0037925243377685547 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- token_embeddings: max diff = 0.007680624723434448\n",
      "- sentence_embedding: max diff = 0.0037925243377685547.\n",
      " The exported model was saved at: setfit-model_opt\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "  --model setfit-model \\\n",
    "  --task feature-extraction \\\n",
    "  --optimize O4 \\\n",
    "  --device cuda \\\n",
    "  setfit-model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-09 22:38:35.504044169 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-09 22:38:35.504069912 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('setfit-model', model_max_length=512)\n",
    "ort_model = ORTModelForFeatureExtraction.from_pretrained('setfit-model_opt', provider=\"CUDAExecutionProvider\",use_io_binding=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "\n",
    "class OnnxSetFitModel:\n",
    "    def __init__(self, ort_model, tokenizer, model_head):\n",
    "        self.ort_model = ort_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_head = model_head\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(self.ort_model.device)\n",
    "\n",
    "        outputs = self.ort_model(**encoded_inputs)\n",
    "        embeddings = mean_pooling(\n",
    "            outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
    "        )\n",
    "        return self.model_head.predict(embeddings.cpu())\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    ").to(ort_model.device)\n",
    "\n",
    "outputs = ort_model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_setfit_model = OnnxSetFitModel(ort_model, tokenizer, model.model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference speed:\n",
      "- Average time for 100 samples: 0.0100 seconds (¬±0.0013)\n",
      "- Samples per second: 9976.33\n",
      "- Time per sample: 0.10 ms\n"
     ]
    }
   ],
   "source": [
    "# Measure inference speed of the trained model\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Prepare a list of sample texts for inference\n",
    "sample_texts = [\"i loved the spiderman movie!\"] * 100  # Create 100 identical samples for consistent measurement\n",
    "\n",
    "# Warm-up run (to avoid measuring initialization overhead)\n",
    "_ = onnx_setfit_model.predict(sample_texts[:5])\n",
    "\n",
    "# Measure inference time\n",
    "num_runs = 100\n",
    "times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    _ = onnx_setfit_model.predict(sample_texts)\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Calculate statistics\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "samples_per_second = len(sample_texts) / avg_time\n",
    "\n",
    "print(f\"Inference speed:\")\n",
    "print(f\"- Average time for {len(sample_texts)} samples: {avg_time:.4f} seconds (¬±{std_time:.4f})\")\n",
    "print(f\"- Samples per second: {samples_per_second:.2f}\")\n",
    "print(f\"- Time per sample: {(avg_time / len(sample_texts)) * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Test Accuracy: 0.4158\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.59      0.39       295\n",
      "           1       0.45      0.51      0.48       830\n",
      "           2       0.53      0.25      0.34       775\n",
      "\n",
      "    accuracy                           0.42      1900\n",
      "   macro avg       0.42      0.45      0.40      1900\n",
      "weighted avg       0.46      0.42      0.41      1900\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "Text: @user first meet December 8th in the Murphy center\n",
      "True label: 1\n",
      "Predicted label: 1\n",
      "--------------------------------------------------\n",
      "Text: \"Sir Terry Leahy, the man behind Tesco's success, is speaking at @user in WGC tomorrow:\n",
      "True label: 1\n",
      "Predicted label: 1\n",
      "--------------------------------------------------\n",
      "Text: @user \" Good morning Kerry! Happy Friday! Wishing you a safe and awesome labor day weekend!\n",
      "True label: 2\n",
      "Predicted label: 1\n",
      "--------------------------------------------------\n",
      "Text: @user There is more Islam in Austria than in Saudi Arabia and the Gulf states. May Allah bless these Austrian folks.@sunnysingh_nw3\n",
      "True label: 2\n",
      "Predicted label: 1\n",
      "--------------------------------------------------\n",
      "Text: Giants 3rd-round pick failed drug test at NFL Combine: Jayron Holsey knew it would hurt him bec... #newyork #sports\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the ONNX model on the test dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get predictions from the ONNX model\n",
    "test_texts = [example[\"text\"] for example in test_dataset]\n",
    "test_labels = [example[\"label\"] for example in test_dataset]\n",
    "\n",
    "# Make predictions using the ONNX model\n",
    "predictions = onnx_setfit_model.predict(test_texts)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"ONNX Model Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(min(5, len(test_texts))):\n",
    "    print(f\"Text: {test_texts[i]}\")\n",
    "    print(f\"True label: {test_labels[i]}\")\n",
    "    print(f\"Predicted label: {predictions[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], shape=(1900,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czech Langauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'query': 'M√°m ji je skvƒõl√°!',\n",
       " 'choices': ['negativn√≠', 'neutr√°ln√≠', 'pozitivn√≠'],\n",
       " 'gold': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cz = load_dataset(\"CZLC/fb_sentiment_balanced\")\n",
    "dataset_cz[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remapping labels in train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3876/3876 [00:00<00:00, 29404.86 examples/s]\n",
      "Remapping labels in validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 597/597 [00:00<00:00, 25536.68 examples/s]\n",
      "Remapping labels in test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [00:00<00:00, 28188.16 examples/s]\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 13729.31 examples/s]\n",
      "***** Running training *****\n",
      "  Num unique pairs = 15000\n",
      "  Batch size = 32\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 03:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.402759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.66}\n",
      "['negativn√≠' 'negativn√≠']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "id2label = {0:'negativn√≠', 1:'neutr√°ln√≠', 2:'pozitivn√≠'}\n",
    "# Remap the labels from integers to strings\n",
    "for split in dataset_cz.keys():\n",
    "    dataset_cz[split] = dataset_cz[split].map(\n",
    "        lambda example: {\"label\": id2label[example[\"gold\"]],\"text\":example[\"query\"]},\n",
    "        desc=f\"Remapping labels in {split} split\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Simulate the few-shot regime by sampling 8 examples per class\n",
    "train_dataset = sample_dataset(dataset_cz[\"train\"], label_column=\"label\", num_samples=50)\n",
    "eval_dataset = dataset_cz[\"validation\"].select(range(100))\n",
    "test_dataset = dataset_cz[\"validation\"].select(range(100, 200))\n",
    "\n",
    "# Load a SetFit model from Hub\n",
    "cz_model = SetFitModel.from_pretrained(\n",
    "    \"Alibaba-NLP/gte-multilingual-base\",\n",
    "    labels=['negativn√≠', 'neutr√°ln√≠', 'pozitivn√≠'],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "args = TrainingArguments(\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cz_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    #column_mapping={\"text\": \"text\", \"label\": \"label\"}  # Map dataset columns to text/label expected by trainer\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "# {'accuracy': 0.8691709844559585}\n",
    "\n",
    "# Push model to the Hub\n",
    "#model = trainer.export_model()\n",
    "\n",
    "# Download from Hub\n",
    " #model = SetFitModel.from_pretrained(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2\")\n",
    "# Run inference\n",
    "preds = cz_model.predict([\"Nepamatuju si z toho v≈Øbec nic!\", \"U≈æ bych na to znovu ne≈°el\"])\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FFit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
